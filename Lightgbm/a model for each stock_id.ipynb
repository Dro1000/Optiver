{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "stock_ids = sorted(\n",
    "    [int(re.sub('stock_id=', '', x)) for x in os.listdir('../book_train.parquet')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = pd.read_csv(\"../train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_id = stock_ids[0]\n",
    "book = pd.read_parquet('../book_train.parquet/stock_id=' + str(stock_id))\n",
    "trades = pd.read_parquet('../trade_train.parquet/stock_id=' + str(stock_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wap1(book):\n",
    "    book['wap1'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) / (\n",
    "              book['ask_size1'] + book['bid_size1'])\n",
    "\n",
    "def add_wap2(book):\n",
    "    book['wap2'] = (book['bid_price2'] * book['ask_size2'] + book['ask_price2'] * book['bid_size2']) / (\n",
    "              book['ask_size2'] + book['bid_size2'])\n",
    "\n",
    "def add_log_return1(book):\n",
    "    book['log_price1'] = np.log(book['wap1'])\n",
    "    book['log_return1'] = book.groupby(['time_id'])['log_price1'].diff()\n",
    "    \n",
    "def add_log_return2(book):\n",
    "    book['log_price2'] = np.log(book['wap2'])\n",
    "    book['log_return2'] = book.groupby(['time_id'])['log_price2'].diff()\n",
    "    \n",
    "def get_vol1(book):\n",
    "    return book.groupby(['time_id'])[['log_return1']].apply(lambda x: np.sum(x**2)**0.5).rename(\n",
    "    {'log_return1': 'vol1'})\n",
    "def get_vol2(book):\n",
    "    return book.groupby(['time_id'])[['log_return2']].apply(lambda x: np.sum(x**2)**0.5).rename(\n",
    "    {'log_return2': 'vol2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_vols(stock_ids, folder):\n",
    "    vols = []\n",
    "    for stock_id in tqdm(stock_ids):\n",
    "        book = pd.read_parquet(folder + 'stock_id=' + str(stock_id))\n",
    "        book = book[book.seconds_in_bucket > 300]\n",
    "        add_wap1(book)\n",
    "        add_wap2(book)\n",
    "        add_log_return1(book)\n",
    "        add_log_return2(book)\n",
    "        vols_temp = pd.concat([get_vol1(book), get_vol2(book)], axis=1).reset_index()\n",
    "        vols_temp['stock_id'] = stock_id\n",
    "        vols.append(vols_temp)\n",
    "    vols = pd.concat(vols)\n",
    "    return vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades['snb_diff'] = trades.groupby('time_id')['seconds_in_bucket'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trades_stats(stock_ids, folder):\n",
    "    trades_stats = []\n",
    "    for stock_id in tqdm(stock_ids):\n",
    "        trades = pd.read_parquet(folder + 'stock_id=' + str(stock_id))\n",
    "        trades['snb_diff'] = trades.groupby('time_id')['seconds_in_bucket'].diff()\n",
    "        #trades = trades[trades['seconds_in_bucket'] > 300]\n",
    "        stats = trades.groupby(['time_id']).agg({\n",
    "            'order_count': sum, 'size': sum, 'snb_diff': np.mean}).reset_index()\n",
    "        stats['stock_id'] = stock_id\n",
    "        trades_stats.append(stats)\n",
    "        \n",
    "    return pd.concat(trades_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [11:42<00:00,  6.27s/it]\n"
     ]
    }
   ],
   "source": [
    "vols = collect_vols(stock_ids, \"../book_train.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [01:16<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "trades_stats = collect_trades_stats(stock_ids, '../trade_train.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(vols, train_targets, on=['time_id', 'stock_id'], how='inner')\n",
    "merged = pd.merge(merged, trades_stats, on=['time_id', 'stock_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "my_scorer = make_scorer(rmspe, greater_is_better=False)\n",
    "def lgb_scorer(y_true, y_pred, weights):\n",
    "    return 'rmspe', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 2/112 [00:00<00:08, 13.51it/s]\u001b[A\n",
      "  4%|▎         | 4/112 [00:00<00:07, 13.71it/s]\u001b[A\n",
      "  5%|▌         | 6/112 [00:00<00:07, 13.78it/s]\u001b[A\n",
      "  7%|▋         | 8/112 [00:00<00:07, 13.63it/s]\u001b[A\n",
      "  9%|▉         | 10/112 [00:00<00:08, 12.14it/s]\u001b[A\n",
      " 11%|█         | 12/112 [00:00<00:07, 12.50it/s]\u001b[A\n",
      " 12%|█▎        | 14/112 [00:01<00:08, 11.98it/s]\u001b[A\n",
      " 14%|█▍        | 16/112 [00:01<00:08, 11.21it/s]\u001b[A\n",
      " 16%|█▌        | 18/112 [00:01<00:08, 10.48it/s]\u001b[A\n",
      " 18%|█▊        | 20/112 [00:01<00:08, 11.22it/s]\u001b[A\n",
      " 20%|█▉        | 22/112 [00:01<00:07, 11.91it/s]\u001b[A\n",
      " 21%|██▏       | 24/112 [00:02<00:07, 11.28it/s]\u001b[A\n",
      " 23%|██▎       | 26/112 [00:02<00:07, 11.71it/s]\u001b[A\n",
      " 25%|██▌       | 28/112 [00:02<00:07, 11.90it/s]\u001b[A\n",
      " 27%|██▋       | 30/112 [00:02<00:06, 12.06it/s]\u001b[A\n",
      " 29%|██▊       | 32/112 [00:02<00:06, 12.35it/s]\u001b[A\n",
      " 30%|███       | 34/112 [00:02<00:06, 12.83it/s]\u001b[A\n",
      " 32%|███▏      | 36/112 [00:02<00:05, 12.92it/s]\u001b[A\n",
      " 34%|███▍      | 38/112 [00:03<00:05, 13.06it/s]\u001b[A\n",
      " 36%|███▌      | 40/112 [00:03<00:05, 13.23it/s]\u001b[A\n",
      " 38%|███▊      | 42/112 [00:03<00:05, 13.21it/s]\u001b[A\n",
      " 39%|███▉      | 44/112 [00:03<00:05, 13.35it/s]\u001b[A\n",
      " 41%|████      | 46/112 [00:03<00:06, 10.81it/s]\u001b[A\n",
      " 43%|████▎     | 48/112 [00:03<00:05, 11.18it/s]\u001b[A\n",
      " 45%|████▍     | 50/112 [00:04<00:05, 11.43it/s]\u001b[A\n",
      " 46%|████▋     | 52/112 [00:04<00:05, 11.01it/s]\u001b[A\n",
      " 48%|████▊     | 54/112 [00:04<00:05, 11.45it/s]\u001b[A\n",
      " 50%|█████     | 56/112 [00:04<00:04, 11.85it/s]\u001b[A\n",
      " 52%|█████▏    | 58/112 [00:04<00:04, 12.55it/s]\u001b[A\n",
      " 54%|█████▎    | 60/112 [00:04<00:04, 12.78it/s]\u001b[A\n",
      " 55%|█████▌    | 62/112 [00:05<00:03, 12.87it/s]\u001b[A\n",
      " 57%|█████▋    | 64/112 [00:05<00:03, 12.07it/s]\u001b[A\n",
      " 59%|█████▉    | 66/112 [00:05<00:04, 11.16it/s]\u001b[A\n",
      " 61%|██████    | 68/112 [00:05<00:03, 11.50it/s]\u001b[A\n",
      " 62%|██████▎   | 70/112 [00:05<00:03, 12.03it/s]\u001b[A\n",
      " 64%|██████▍   | 72/112 [00:05<00:03, 12.56it/s]\u001b[A\n",
      " 66%|██████▌   | 74/112 [00:06<00:02, 12.96it/s]\u001b[A\n",
      " 68%|██████▊   | 76/112 [00:06<00:02, 13.33it/s]\u001b[A\n",
      " 70%|██████▉   | 78/112 [00:06<00:02, 13.33it/s]\u001b[A\n",
      " 71%|███████▏  | 80/112 [00:06<00:02, 12.56it/s]\u001b[A\n",
      " 73%|███████▎  | 82/112 [00:06<00:02, 12.63it/s]\u001b[A\n",
      " 75%|███████▌  | 84/112 [00:06<00:02, 12.76it/s]\u001b[A\n",
      " 77%|███████▋  | 86/112 [00:07<00:02, 12.35it/s]\u001b[A\n",
      " 79%|███████▊  | 88/112 [00:07<00:01, 12.68it/s]\u001b[A\n",
      " 80%|████████  | 90/112 [00:07<00:01, 11.91it/s]\u001b[A\n",
      " 82%|████████▏ | 92/112 [00:07<00:01, 12.15it/s]\u001b[A\n",
      " 84%|████████▍ | 94/112 [00:07<00:01, 12.52it/s]\u001b[A\n",
      " 86%|████████▌ | 96/112 [00:07<00:01, 13.06it/s]\u001b[A\n",
      " 88%|████████▊ | 98/112 [00:07<00:01, 13.09it/s]\u001b[A\n",
      " 89%|████████▉ | 100/112 [00:08<00:00, 12.98it/s]\u001b[A\n",
      " 91%|█████████ | 102/112 [00:08<00:00, 12.84it/s]\u001b[A\n",
      " 93%|█████████▎| 104/112 [00:08<00:00, 10.50it/s]\u001b[A\n",
      " 95%|█████████▍| 106/112 [00:08<00:00,  9.25it/s]\u001b[A\n",
      " 96%|█████████▋| 108/112 [00:09<00:00,  7.37it/s]\u001b[A\n",
      " 97%|█████████▋| 109/112 [00:09<00:00,  6.40it/s]\u001b[A\n",
      " 98%|█████████▊| 110/112 [00:09<00:00,  6.31it/s]\u001b[A\n",
      " 99%|█████████▉| 111/112 [00:09<00:00,  6.31it/s]\u001b[A\n",
      "100%|██████████| 112/112 [00:09<00:00, 11.25it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "for stock_id in tqdm(stock_ids):\n",
    "    cols = ['log_return1', 'log_return2', 'order_count','size', 'snb_diff']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        merged[merged.stock_id == stock_id][cols], merged[merged.stock_id == stock_id]['target'])\n",
    "    \n",
    "    clf = lgb.LGBMRegressor(num_leaves=7, learning_rate=0.05, n_estimators=100)\n",
    "    clf.fit(X_train, y_train, sample_weight=1 / np.square(y_train))\n",
    "    \n",
    "    res[stock_id] = ((clf, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answers = []\n",
    "test_true = []\n",
    "for stock_id in res:\n",
    "    reg = res[stock_id][0]\n",
    "    X_test = res[stock_id][2]\n",
    "    y_test = res[stock_id][4]\n",
    "    test_answers.append(reg.predict(X_test))\n",
    "    test_true.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answers = []\n",
    "train_true = []\n",
    "for stock_id in res:\n",
    "    reg = res[stock_id][0]\n",
    "    X_train = res[stock_id][1]\n",
    "    y_train = res[stock_id][3]\n",
    "    test_answers.append(reg.predict(X_train))\n",
    "    test_true.append(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.concatenate(test_true)\n",
    "y_pred = np.concatenate(test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26197571718962925"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmspe(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=20, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['log_return1', 'log_return2', 'order_count','size', 'snb_diff']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    merged[merged.stock_id == stock_id][cols], merged[merged.stock_id == stock_id]['target'])\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=20, max_depth=5)\n",
    "clf.fit(X_train, y_train, sample_weight=1 / np.square(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25126617264369977"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestRegressor(n_estimators=100, max_depth=4)\n",
    "clf.fit(X_train, y_train, sample_weight=1 / np.square(y_train))\n",
    "rmspe(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25208127188004"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = lgb.LGBMRegressor(max_depth=3, learning_rate=0.05, n_estimators=300)\n",
    "clf.fit(X_train, y_train, sample_weight=1 / np.square(y_train))\n",
    "rmspe(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmspe(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
